{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7307781,
          "sourceType": "datasetVersion",
          "datasetId": 4240176
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Analisis Sentimen Pengguna shopee Dengan SVM",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchmadLutfi196/23059-datamining/blob/main/Analisis_Sentimen_Ulasan_Tiktok_Dengan_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "alvianardiansyah_dataset_ulasan_pengguna_shopee_path = kagglehub.dataset_download('alvianardiansyah/dataset-ulasan-pengguna-shopee')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ZzKTsU4i06qE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Sentimen Pengguna Aplikasi Shopee di Google Play Store Terkait Fitur COD dengan Menggunakan Metode SVM****"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-12-20T15:56:50.320371Z",
          "iopub.execute_input": "2023-12-20T15:56:50.320774Z",
          "iopub.status.idle": "2023-12-20T15:57:07.379596Z",
          "shell.execute_reply.started": "2023-12-20T15:56:50.320738Z",
          "shell.execute_reply": "2023-12-20T15:57:07.378227Z"
        },
        "id": "ZPySK0-106qJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review pada Google Play Store merupakan aspek penting yang digunakan pengguna untuk mengekspresikan penilaian mereka terhadap sebuah aplikasi. Aplikasi Shopee adalah platform e-commerce yang memungkinkan pengguna untuk melakukan berbagai aktivitas, seperti berbelanja, menjual produk, dan berinteraksi dengan penjual maupun pembeli lainnya. Aplikasi ini menyediakan fitur-fitur seperti penawaran menarik, sistem pembayaran yang mudah, layanan pelanggan, dan berbagai promosi.\n",
        "\n",
        "Dalam sebuah penelitian, dilakukan proses pengumpulan data dari ulasan pengguna aplikasi Shopee melalui tahap scraping. Selanjutnya, data tersebut dilakukan pelabelan dengan mengkategorikan ulasan ke dalam kelas positif, negatif, dan netral. Kemudian, dilakukan text preprocessing untuk mengolah data dengan membersihkan, menyeleksi, dan mengubahnya menjadi data yang lebih terstruktur agar dapat diolah lebih lanjut sesuai kebutuhan penelitian.\n",
        "\n",
        "Setelah mendapatkan data hasil text preprocessing, langkah berikutnya adalah melakukan pembobotan kata menggunakan metode Term Frequency â€“ Inverse Document Frequency (TF-IDF). Metode ini memberikan nilai pada setiap kata dalam sebuah dokumen berdasarkan frekuensi kemunculan kata tersebut dalam seluruh dokumen yang ada.\n",
        "\n",
        "Selanjutnya, data yang telah diolah menjalani proses klasifikasi menggunakan algoritma Support Vector Machine (SVM). SVM digunakan untuk mengklasifikasikan ulasan menjadi kategori sentimen yang tepat, seperti positif, negatif, atau netral berdasarkan pola-pola yang teridentifikasi dari data ulasan yang telah diproses sebelumnya."
      ],
      "metadata": {
        "id": "DRopFiaR06qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manfaat dilakukan analisis sentimen adalah:\n",
        "\n",
        "1. Hasil analisis sentimen dapat dilihat oleh pengguna aplikasi Shopee dan digunakan untuk memahami opini pengguna lain.\n",
        "\n",
        "2. Sebagai bahan pertimbangan bagi Shopee untuk meningkatkan dan menyempurnakan platformnya berdasarkan hasil analisis sentimen yang diperoleh.\n",
        "\n",
        "    Dengan mengubah aplikasi yang disebutkan dari MyXL menjadi Shopee, fokusnya berpindah dari platform layanan telekomunikasi ke platform e-commerce. Dalam konteks ini, analisis sentimen tetap relevan untuk memahami opini pengguna dan memperbaiki layanan yang disediakan oleh Shopee."
      ],
      "metadata": {
        "id": "lKkzGx5t06qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Scraping Data\n",
        "\n",
        "Scraping data dilakukan dengan mengumpukan data ulasan pengguna aplikasi Shopee dari Google Play Store menggunakan bantuan dari library google-play-scraper dan IDE Google Collab. Data yang diambil berupa nama pemberi ulasan, nilai bintang yang diberikan, waktu ulasan dikirim dan isi review ulasan."
      ],
      "metadata": {
        "id": "K0DM1Z8506qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-play-scraper"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:28:24.43457Z",
          "iopub.execute_input": "2024-01-05T10:28:24.434989Z",
          "iopub.status.idle": "2024-01-05T10:28:40.524074Z",
          "shell.execute_reply.started": "2024-01-05T10:28:24.434956Z",
          "shell.execute_reply": "2024-01-05T10:28:40.522911Z"
        },
        "trusted": true,
        "id": "Cn7VPeKv06qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import app\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google_play_scraper import Sort, reviews"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:28:40.526397Z",
          "iopub.execute_input": "2024-01-05T10:28:40.52682Z",
          "iopub.status.idle": "2024-01-05T10:28:41.010924Z",
          "shell.execute_reply.started": "2024-01-05T10:28:40.526782Z",
          "shell.execute_reply": "2024-01-05T10:28:41.009893Z"
        },
        "trusted": true,
        "id": "9qnrTEVR06qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, continuation_token = reviews(\n",
        "    'com.shopee.id',\n",
        "    lang='id',\n",
        "    country='id',\n",
        "    sort=Sort.MOST_RELEVANT,\n",
        "    count=50000,\n",
        "    filter_score_with=None\n",
        ")\n",
        "df_shopee = pd.DataFrame(np.array(result), columns=['review'])\n",
        "df_shopee = df_shopee.join(pd.DataFrame(df_shopee.pop('review').tolist()))\n",
        "df_shopee_cod = df_shopee[df_shopee['content'].str.contains('COD', case=False)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:28:41.012072Z",
          "iopub.execute_input": "2024-01-05T10:28:41.01252Z",
          "iopub.status.idle": "2024-01-05T10:32:12.897562Z",
          "shell.execute_reply.started": "2024-01-05T10:28:41.012493Z",
          "shell.execute_reply": "2024-01-05T10:32:12.89657Z"
        },
        "trusted": true,
        "id": "cvrrbyEQ06qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_shopee_cod.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:12.899076Z",
          "iopub.execute_input": "2024-01-05T10:32:12.900098Z",
          "iopub.status.idle": "2024-01-05T10:32:12.924818Z",
          "shell.execute_reply.started": "2024-01-05T10:32:12.900062Z",
          "shell.execute_reply": "2024-01-05T10:32:12.923658Z"
        },
        "trusted": true,
        "id": "D2xtre4F06qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df_shopee_cod[['userName', 'score','at', 'content']]\n",
        "sorted_df = new_df.sort_values(by='at', ascending=False)\n",
        "sorted_df.to_csv(\"Data ulasan Shopee tentang COD.csv\", index = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:12.927993Z",
          "iopub.execute_input": "2024-01-05T10:32:12.92837Z",
          "iopub.status.idle": "2024-01-05T10:32:12.984626Z",
          "shell.execute_reply.started": "2024-01-05T10:32:12.928327Z",
          "shell.execute_reply": "2024-01-05T10:32:12.983555Z"
        },
        "trusted": true,
        "id": "vgFWADW_06qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Case Folding\n",
        "\n",
        "Case folding dilakukan pengubahan seluruh huruf menjadi kecil (lowercase) yang ada pada dokumen. Tahap ini akan dibantu dengan bantuan library RegEx."
      ],
      "metadata": {
        "id": "htBpT1E406qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def cleaningulasan(content):\n",
        "    ulasan = re.sub(r'@[A-Za-a0-9]+',' ', content)\n",
        "    ulasan = re.sub(r'#[A-Za-z0-9]+',' ', content)\n",
        "    ulasan = re.sub(r\"http\\S+\",' ', content)\n",
        "    ulasan = re.sub(r'[0-9]+',' ', content)\n",
        "    ulasan = re.sub(r\"[-()\\\"#/@;:<>{}'+=~|.!?,_]\", \" \", content)\n",
        "    ulasan = content.strip(' ')\n",
        "    return ulasan\n",
        "\n",
        "def clearEmoji(content):\n",
        "    return content.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "def replaceTOM(content):\n",
        "    pola = re.compile(r'(.)\\1{2,}', re.DOTALL)\n",
        "    return pola.sub(r'\\1', content)\n",
        "\n",
        "def casefoldingText(content):\n",
        "    ulasan = content.lower()\n",
        "    return ulasan\n",
        "\n",
        "# Mendefinisikan DataFrame baru dengan kolom yang dipilih\n",
        "new_df_copy = new_df.copy()\n",
        "\n",
        "# Membersihkan konten menggunakan fungsi yang telah dibuat\n",
        "new_df_copy['Cleaning'] = new_df_copy['content'].apply(cleaningulasan)\n",
        "new_df_copy['HapusEmoji'] = new_df_copy['Cleaning'].apply(clearEmoji)\n",
        "new_df_copy['3/Lebih'] = new_df_copy['HapusEmoji'].apply(replaceTOM)\n",
        "new_df_copy['CaseFolding'] = new_df_copy['3/Lebih'].apply(casefoldingText)\n",
        "\n",
        "# Mengurutkan DataFrame berdasarkan kolom 'at'\n",
        "sorted_df = new_df_copy.sort_values(by='at', ascending=False)\n",
        "\n",
        "# Menyimpan DataFrame yang telah diolah ke dalam file CSV\n",
        "sorted_df.to_csv(\"Data ulasan Shopee tentang COD.csv\", index=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:12.986368Z",
          "iopub.execute_input": "2024-01-05T10:32:12.98674Z",
          "iopub.status.idle": "2024-01-05T10:32:13.275277Z",
          "shell.execute_reply.started": "2024-01-05T10:32:12.986711Z",
          "shell.execute_reply": "2024-01-05T10:32:13.27421Z"
        },
        "trusted": true,
        "id": "jflZ_QGO06qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_copy.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:13.276749Z",
          "iopub.execute_input": "2024-01-05T10:32:13.277841Z",
          "iopub.status.idle": "2024-01-05T10:32:13.295654Z",
          "shell.execute_reply.started": "2024-01-05T10:32:13.277805Z",
          "shell.execute_reply": "2024-01-05T10:32:13.293742Z"
        },
        "trusted": true,
        "id": "as18LLge06qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pelabelan Dataset\n",
        "\n",
        "Pelabelan dataset dilakukan secara manual terhadap pelabelan data kedalam kelas kategori positif, negatif atau netral. Proses ini akan dilakukan dengan bantuan ahli bahasa Indonesia dan dilakukan pelabelan oleh setidaknya lebih dari dua orang."
      ],
      "metadata": {
        "id": "6o6hJZ2Q06qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = []\n",
        "for index, row in new_df.iterrows():\n",
        "  if row['score'] == 1 or row['score'] == 2:\n",
        "    label.append('Negatif')\n",
        "  elif row['score'] == 3:\n",
        "    label.append('Netral')\n",
        "  else:\n",
        "    label.append('Positif')\n",
        "new_df_copy['sentimen'] = label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:13.297742Z",
          "iopub.execute_input": "2024-01-05T10:32:13.298663Z",
          "iopub.status.idle": "2024-01-05T10:32:13.451027Z",
          "shell.execute_reply.started": "2024-01-05T10:32:13.298625Z",
          "shell.execute_reply": "2024-01-05T10:32:13.449943Z"
        },
        "trusted": true,
        "id": "xniWT1J606qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_copy.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:13.452774Z",
          "iopub.execute_input": "2024-01-05T10:32:13.453238Z",
          "iopub.status.idle": "2024-01-05T10:32:13.471076Z",
          "shell.execute_reply.started": "2024-01-05T10:32:13.453196Z",
          "shell.execute_reply": "2024-01-05T10:32:13.470261Z"
        },
        "trusted": true,
        "id": "Fk3AHE3C06qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tokenizing\n",
        "\n",
        "Hasil proses case folding, kalimat akan diproses dengan menguraikannya menjadi token-token atau kata-kata. Pada tahap ini akan dibantu dengan library NLTK."
      ],
      "metadata": {
        "id": "IV9fC3_w06qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenizingText(content):\n",
        "    ulasan = word_tokenize(content)\n",
        "    return ulasan\n",
        "\n",
        "# Menerapkan tokenizingText pada setiap elemen di kolom 'CaseFolding'\n",
        "new_df_copy['Tokenizing'] = new_df_copy['CaseFolding'].apply(tokenizingText)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:13.472396Z",
          "iopub.execute_input": "2024-01-05T10:32:13.472713Z",
          "iopub.status.idle": "2024-01-05T10:32:15.440313Z",
          "shell.execute_reply.started": "2024-01-05T10:32:13.472686Z",
          "shell.execute_reply": "2024-01-05T10:32:15.43887Z"
        },
        "trusted": true,
        "id": "RZkoa1XA06qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_copy.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:15.442105Z",
          "iopub.execute_input": "2024-01-05T10:32:15.443489Z",
          "iopub.status.idle": "2024-01-05T10:32:15.468129Z",
          "shell.execute_reply.started": "2024-01-05T10:32:15.44344Z",
          "shell.execute_reply": "2024-01-05T10:32:15.46684Z"
        },
        "trusted": true,
        "id": "HnaL-Qck06qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Formalisasi\n",
        "\n",
        "Tahap formalisasi dilakukan untuk pengubah penggunaan kata tidak baku menjadi baku sesuai dengan KBBI. Proses akan menggunakan file dataset slangwords yang berisi kata slang yang nanti akan diubah menjadi baku. Tahap ini dibantu dengan library RegEx."
      ],
      "metadata": {
        "id": "qEUw_sW606qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToSlangword(tokens):\n",
        "    # Load slang word dictionary from a file\n",
        "    with open(\"/kaggle/input/d/alvianardiansyah/dataset-ulasan-pengguna-shopee/slangwords.txt\", \"r\") as file:\n",
        "        kamusSlang = eval(file.read())\n",
        "\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(kamusSlang.keys()) + r')\\b')\n",
        "    content = []\n",
        "\n",
        "    for kata in tokens:\n",
        "        filterSlang = pattern.sub(lambda x: kamusSlang[x.group()], kata)\n",
        "        content.append(filterSlang.lower())\n",
        "\n",
        "    return content\n",
        "\n",
        "# Assuming 'new_df' contains the data\n",
        "new_df_copy['Formalisasi'] = new_df_copy['Tokenizing'].apply(convertToSlangword)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:15.469664Z",
          "iopub.execute_input": "2024-01-05T10:32:15.470077Z",
          "iopub.status.idle": "2024-01-05T10:32:37.14506Z",
          "shell.execute_reply.started": "2024-01-05T10:32:15.470045Z",
          "shell.execute_reply": "2024-01-05T10:32:37.143992Z"
        },
        "trusted": true,
        "id": "fkw10QQo06qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_copy.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:37.146302Z",
          "iopub.execute_input": "2024-01-05T10:32:37.146633Z",
          "iopub.status.idle": "2024-01-05T10:32:37.205601Z",
          "shell.execute_reply.started": "2024-01-05T10:32:37.146593Z",
          "shell.execute_reply": "2024-01-05T10:32:37.204495Z"
        },
        "trusted": true,
        "id": "ebWAbb7I06qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Stopword Removal\n",
        "\n",
        "Hasil proses formalisasi kemudian akan dilakukan seleksi kata yang tidak penting dan menghapus kata tersebut. Tahap ini dibantu dengan library NLTK."
      ],
      "metadata": {
        "id": "SyAbo--K06qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "daftar_stopword = stopwords.words('indonesian')\n",
        "# Append additional stopwords\n",
        "daftar_stopword.extend([\"yg\", \"dg\", \"rt\"])\n",
        "daftar_stopword = set(daftar_stopword)\n",
        "\n",
        "def stopwordText(words):\n",
        "    return [word for word in words if word not in daftar_stopword]\n",
        "\n",
        "# Assuming 'new_df' contains the data and 'Tokenizing' column has the tokenized words\n",
        "new_df_copy['WithoutStopwords'] = new_df_copy['Tokenizing'].apply(stopwordText)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:37.211692Z",
          "iopub.execute_input": "2024-01-05T10:32:37.212439Z",
          "iopub.status.idle": "2024-01-05T10:32:37.244801Z",
          "shell.execute_reply.started": "2024-01-05T10:32:37.212404Z",
          "shell.execute_reply": "2024-01-05T10:32:37.243696Z"
        },
        "trusted": true,
        "id": "810B3sl806qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_copy.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:37.246022Z",
          "iopub.execute_input": "2024-01-05T10:32:37.246351Z",
          "iopub.status.idle": "2024-01-05T10:32:37.27893Z",
          "shell.execute_reply.started": "2024-01-05T10:32:37.246323Z",
          "shell.execute_reply": "2024-01-05T10:32:37.277702Z"
        },
        "trusted": true,
        "id": "oiOc0bjN06qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Stemming\n",
        "\n",
        "Proses stemming dilakukan perubahan kata yang berimbuhan menjadi kata dasar. Tahap ini dibantu dengan library Sastrawi dan Swifter."
      ],
      "metadata": {
        "id": "CB1AeSxv06qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install swifter\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import swifter\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in new_df_copy['WithoutStopwords']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "\n",
        "def stemmingText(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "new_df_copy['Stemming'] = new_df_copy['WithoutStopwords'].swifter.apply(stemmingText)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:32:37.280217Z",
          "iopub.execute_input": "2024-01-05T10:32:37.280518Z",
          "iopub.status.idle": "2024-01-05T10:42:21.06097Z",
          "shell.execute_reply.started": "2024-01-05T10:32:37.280493Z",
          "shell.execute_reply": "2024-01-05T10:42:21.05995Z"
        },
        "trusted": true,
        "id": "2wML8UjC06qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_copy.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:21.062513Z",
          "iopub.execute_input": "2024-01-05T10:42:21.063556Z",
          "iopub.status.idle": "2024-01-05T10:42:21.1013Z",
          "shell.execute_reply.started": "2024-01-05T10:42:21.063511Z",
          "shell.execute_reply": "2024-01-05T10:42:21.099998Z"
        },
        "trusted": true,
        "id": "zHtSQALZ06qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Visualisasi Data (NLP)\n",
        "Visualisasi data dalam pemrosesan bahasa alami (Natural Language Processing/NLP) melibatkan representasi grafis dari informasi yang terdapat dalam teks atau data teks yang telah diproses. Ini membantu dalam pemahaman, analisis, dan komunikasi hasil dari algoritma pemrosesan bahasa alami."
      ],
      "metadata": {
        "id": "gbm44U8y06qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jika new_df_copy adalah salinan yang ingin dimodifikasi\n",
        "train_s0 = new_df_copy[new_df_copy['sentimen'] == 'Negatif'].copy()\n",
        "\n",
        "# Mengisi nilai yang hilang pada kolom 'content' untuk baris yang terfilter\n",
        "train_s0.loc[:, 'content'] = train_s0['content'].fillna(\"tidak ada komentar\")\n",
        "\n",
        "# Menampilkan beberapa baris pertama dari DataFrame yang telah dimodifikasi\n",
        "train_s0.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:21.10282Z",
          "iopub.execute_input": "2024-01-05T10:42:21.103162Z",
          "iopub.status.idle": "2024-01-05T10:42:21.153758Z",
          "shell.execute_reply.started": "2024-01-05T10:42:21.103133Z",
          "shell.execute_reply": "2024-01-05T10:42:21.152436Z"
        },
        "trusted": true,
        "id": "PFPAU2Ln06qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:21.155506Z",
          "iopub.execute_input": "2024-01-05T10:42:21.155924Z",
          "iopub.status.idle": "2024-01-05T10:42:21.222Z",
          "shell.execute_reply.started": "2024-01-05T10:42:21.155888Z",
          "shell.execute_reply": "2024-01-05T10:42:21.220737Z"
        },
        "trusted": true,
        "id": "m7DYpGt_06qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s0 = ' '.join(word for word in train_s0['content'])\n",
        "wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.title(\"Sentimen Negatif\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:21.223828Z",
          "iopub.execute_input": "2024-01-05T10:42:21.224582Z",
          "iopub.status.idle": "2024-01-05T10:42:25.130511Z",
          "shell.execute_reply.started": "2024-01-05T10:42:21.224526Z",
          "shell.execute_reply": "2024-01-05T10:42:25.128932Z"
        },
        "trusted": true,
        "id": "1SsXNCLV06qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_s1 = new_df_copy[new_df_copy['sentimen'] == 'Netral'].copy()\n",
        "train_s1.loc[:, 'content'] = train_s1['content'].fillna(\"tidak ada komentar\")\n",
        "train_s1.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:25.132178Z",
          "iopub.execute_input": "2024-01-05T10:42:25.132698Z",
          "iopub.status.idle": "2024-01-05T10:42:25.17887Z",
          "shell.execute_reply.started": "2024-01-05T10:42:25.132653Z",
          "shell.execute_reply": "2024-01-05T10:42:25.177703Z"
        },
        "trusted": true,
        "id": "SpC_j0MV06q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s1 = ' '.join(word for word in train_s1['content'])\n",
        "wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s1)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.title(\"Sentimen Netral\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:25.180489Z",
          "iopub.execute_input": "2024-01-05T10:42:25.181725Z",
          "iopub.status.idle": "2024-01-05T10:42:28.179533Z",
          "shell.execute_reply.started": "2024-01-05T10:42:25.181676Z",
          "shell.execute_reply": "2024-01-05T10:42:28.178325Z"
        },
        "trusted": true,
        "id": "cZ-rHSJz06q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_s2 = new_df_copy[new_df_copy['sentimen'] == 'Positif'].copy()\n",
        "train_s2.loc[:, 'content'] = train_s2['content'].fillna(\"tidak ada komentar\")\n",
        "train_s2.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:28.181389Z",
          "iopub.execute_input": "2024-01-05T10:42:28.181842Z",
          "iopub.status.idle": "2024-01-05T10:42:28.228015Z",
          "shell.execute_reply.started": "2024-01-05T10:42:28.181802Z",
          "shell.execute_reply": "2024-01-05T10:42:28.226812Z"
        },
        "trusted": true,
        "id": "sWNXABZP06q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s2 = ' '.join(word for word in train_s2['content'])\n",
        "wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s2)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.title(\"Sentimen Positif\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:28.229511Z",
          "iopub.execute_input": "2024-01-05T10:42:28.229916Z",
          "iopub.status.idle": "2024-01-05T10:42:31.220652Z",
          "shell.execute_reply.started": "2024-01-05T10:42:28.229882Z",
          "shell.execute_reply": "2024-01-05T10:42:31.219431Z"
        },
        "trusted": true,
        "id": "LuhmYI3g06q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimen_data = pd.value_counts(new_df_copy['sentimen'], sort=True)\n",
        "sentimen_data.plot(kind='bar', color=['red', 'green', 'lightskyblue'])\n",
        "plt.title(\"Bar Chart\")\n",
        "plt.show"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:31.222342Z",
          "iopub.execute_input": "2024-01-05T10:42:31.222905Z",
          "iopub.status.idle": "2024-01-05T10:42:31.526156Z",
          "shell.execute_reply.started": "2024-01-05T10:42:31.222863Z",
          "shell.execute_reply": "2024-01-05T10:42:31.524683Z"
        },
        "trusted": true,
        "id": "BB8IZw3f06q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Pembobotan Kata dengan TF-IDF\n",
        "\n",
        "Pada tahap ini dilakukan pembobotan kata dari hasil stemming dengan metode Term Inverse Document Frequency (TF-IDF).Metode TF-IDF digunakan untuk mengetahui seberapa sering suatu kata muncul di dalam dokumen. Tahap ini dibantu dengan library sklearn."
      ],
      "metadata": {
        "id": "ecsGnN8g06q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Pastikan new_df berisi kolom 'Stemming' dan 'sentimen'\n",
        "X = new_df_copy['Stemming']\n",
        "Y = new_df_copy['sentimen']\n",
        "\n",
        "# Misalkan x_train dan x_test adalah list token yang ingin digabungkan kembali menjadi teks\n",
        "x_text = [' '.join(tokens) for tokens in X]\n",
        "\n",
        "# Sekarang x_text berisi teks yang bisa digunakan untuk fit_transform\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_text, Y, test_size=0.2)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "x_train = vectorizer.fit_transform(x_train)\n",
        "x_test = vectorizer.transform(x_test)\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "y_train = Encoder.fit_transform(y_train)\n",
        "y_test = Encoder.transform(y_test)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:31.527732Z",
          "iopub.execute_input": "2024-01-05T10:42:31.528119Z",
          "iopub.status.idle": "2024-01-05T10:42:31.643311Z",
          "shell.execute_reply.started": "2024-01-05T10:42:31.528086Z",
          "shell.execute_reply": "2024-01-05T10:42:31.642111Z"
        },
        "trusted": true,
        "id": "yHeY_FkM06q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train dan x_test telah di-vektorisasi menggunakan TfidfVectorizer\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# x_train = vectorizer.fit_transform(x_train)\n",
        "# x_test = vectorizer.transform(x_test)\n",
        "\n",
        "# Hitung total kemunculan setiap kata di semua dokumen\n",
        "word_freq = x_train.sum(axis=0)\n",
        "\n",
        "# Dapatkan indeks dari fitur kata pada vektorisasi\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Buat daftar kata beserta jumlah kemunculannya\n",
        "word_freq_list = [(word, word_freq[0, idx]) for word, idx in zip(feature_names, range(len(feature_names)))]\n",
        "\n",
        "# Urutkan berdasarkan jumlah kemunculan kata\n",
        "word_freq_list = sorted(word_freq_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Tampilkan beberapa kata beserta jumlah kemunculannya\n",
        "num_words_to_display = 10  # Ganti dengan jumlah kata yang ingin ditampilkan\n",
        "for word, freq in word_freq_list[:num_words_to_display]:\n",
        "    print(f\"Kata: {word}, Kemunculan: {freq}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T10:42:31.645097Z",
          "iopub.execute_input": "2024-01-05T10:42:31.645564Z",
          "iopub.status.idle": "2024-01-05T10:42:31.67215Z",
          "shell.execute_reply.started": "2024-01-05T10:42:31.645532Z",
          "shell.execute_reply": "2024-01-05T10:42:31.670799Z"
        },
        "trusted": true,
        "id": "fPmGi1db06rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Klasifikasi dengan SVM\n",
        "\n",
        "Klasifikasi ulasan pengguna dilakukan menggunakan algoritma Support Vector Machine yang akan dibantu dengan library Scikit-Learn. Proses klasifikasi menggunakan nilai data latih dan data uji sebesar 80%:20%, dilakukan percobaan sebanyak 5x dan kernel akan menggunakan kernel rbf dan kernel linear."
      ],
      "metadata": {
        "id": "LDiguTN206rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import svm\n",
        "\n",
        "SVM = svm.SVC(kernel='rbf')    #Jika dengan Kernel RBF\n",
        "SVM.fit(x_train,y_train)\n",
        "\n",
        "acc_score_rbf = cross_val_score(SVM, x_train, y_train, cv=5, scoring='accuracy')\n",
        "pre_score_rbf = cross_val_score(SVM, x_train, y_train, cv=5, scoring='precision_macro')\n",
        "rec_score_rbf = cross_val_score(SVM, x_train, y_train, cv=5, scoring='recall_macro')\n",
        "f_score_rbf = cross_val_score(SVM, x_train, y_train, cv=5, scoring='f1_macro')\n",
        "\n",
        "print('Hasil Accuracy : %s' % (acc_score_rbf))\n",
        "print('Hasil Rata - Rata Accuracy : %s' % acc_score_rbf.mean())\n",
        "print('Hasil Precision : %s' % (pre_score_rbf))\n",
        "print('Hasil Rata - Rata Precision : %s' % pre_score_rbf.mean())\n",
        "print('Hasil Recall : %s' % (rec_score_rbf))\n",
        "print('Hasil Rata - Rata Recall : %s' % rec_score_rbf.mean())\n",
        "print('Hasil F-Measure : %s' % (f_score_rbf))\n",
        "print('Hasil Rata - Rata F-Measure : %s' % f_score_rbf.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T11:00:32.89424Z",
          "iopub.execute_input": "2024-01-05T11:00:32.894753Z",
          "iopub.status.idle": "2024-01-05T11:00:44.925533Z",
          "shell.execute_reply.started": "2024-01-05T11:00:32.894718Z",
          "shell.execute_reply": "2024-01-05T11:00:44.92447Z"
        },
        "trusted": true,
        "id": "nX80NySv06rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import svm\n",
        "\n",
        "SVM = svm.SVC(kernel='linear') #Jika dengan Kernel Linear\n",
        "SVM.fit(x_train,y_train)\n",
        "\n",
        "acc_score_linear = cross_val_score(SVM, x_train, y_train, cv=5, scoring='accuracy')\n",
        "pre_score_linear = cross_val_score(SVM, x_train, y_train, cv=5, scoring='precision_macro')\n",
        "rec_score_linear = cross_val_score(SVM, x_train, y_train, cv=5, scoring='recall_macro')\n",
        "f_score_linear = cross_val_score(SVM, x_train, y_train, cv=5, scoring='f1_macro')\n",
        "\n",
        "print('Hasil Accuracy : %s' % (acc_score_linear))\n",
        "print('Hasil Rata - Rata Accuracy : %s' % acc_score_linear.mean())\n",
        "print('Hasil Precision : %s' % (pre_score_linear))\n",
        "print('Hasil Rata - Rata Precision : %s' % pre_score_linear.mean())\n",
        "print('Hasil Recall : %s' % (rec_score_linear))\n",
        "print('Hasil Rata - Rata Recall : %s' % rec_score_linear.mean())\n",
        "print('Hasil F-Measure : %s' % (f_score_linear))\n",
        "print('Hasil Rata - Rata F-Measure : %s' % f_score_linear.mean())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T11:02:02.108044Z",
          "iopub.execute_input": "2024-01-05T11:02:02.108654Z",
          "iopub.status.idle": "2024-01-05T11:02:10.830515Z",
          "shell.execute_reply.started": "2024-01-05T11:02:02.108587Z",
          "shell.execute_reply": "2024-01-05T11:02:10.829371Z"
        },
        "trusted": true,
        "id": "88touV8G06rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Data evaluasi yang ingin diplot\n",
        "scores = {\n",
        "    'Accuracy': acc_score_rbf.mean(),\n",
        "    'Precision': pre_score_rbf.mean(),\n",
        "    'Recall': rec_score_rbf.mean(),\n",
        "    'F-Measure': f_score_rbf.mean()\n",
        "}\n",
        "\n",
        "# Membuat DataFrame dari data evaluasi\n",
        "eval_df = pd.DataFrame.from_dict(scores, orient='index', columns=['Score'])\n",
        "\n",
        "# Plot grafik garis Linear\n",
        "eval_df['Score'].plot(kind='line', marker='o', color='blue')  # Grafik garis dengan penanda titik biru\n",
        "plt.title('Hasil Evaluasi Model RBF')\n",
        "plt.xlabel('Metrik')\n",
        "plt.ylabel('Nilai Rata-Rata')\n",
        "plt.grid(True)  # Menambahkan grid\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T11:03:06.567578Z",
          "iopub.execute_input": "2024-01-05T11:03:06.56812Z",
          "iopub.status.idle": "2024-01-05T11:03:06.859288Z",
          "shell.execute_reply.started": "2024-01-05T11:03:06.568084Z",
          "shell.execute_reply": "2024-01-05T11:03:06.858482Z"
        },
        "trusted": true,
        "id": "RxIpswYO06rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Data evaluasi yang ingin diplot\n",
        "scores = {\n",
        "    'Accuracy': acc_score_linear.mean(),\n",
        "    'Precision': pre_score_linear.mean(),\n",
        "    'Recall': rec_score_linear.mean(),\n",
        "    'F-Measure': f_score_linear.mean()\n",
        "}\n",
        "\n",
        "# Membuat DataFrame dari data evaluasi\n",
        "eval_df = pd.DataFrame.from_dict(scores, orient='index', columns=['Score'])\n",
        "\n",
        "# Plot grafik garis Linear\n",
        "eval_df['Score'].plot(kind='line', marker='o', color='blue')  # Grafik garis dengan penanda titik biru\n",
        "plt.title('Hasil Evaluasi Model Linear')\n",
        "plt.xlabel('Metrik')\n",
        "plt.ylabel('Nilai Rata-Rata')\n",
        "plt.grid(True)  # Menambahkan grid\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T11:03:45.577637Z",
          "iopub.execute_input": "2024-01-05T11:03:45.578478Z",
          "iopub.status.idle": "2024-01-05T11:03:45.861725Z",
          "shell.execute_reply.started": "2024-01-05T11:03:45.578426Z",
          "shell.execute_reply": "2024-01-05T11:03:45.860661Z"
        },
        "trusted": true,
        "id": "FMHw3h1c06rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hasil Klasifikasi\n",
        "\n",
        "Hasil klasifikasi didapatkan bahwa pada pengujian pertama penggunaan data utuh berjumlah 1840 data didapatkan bahwa hasil akurasi terbaik pada data utuh dengan pengujian kernel Linear menghasilkan nilai akurasi sebesar 72,37%. Sedangkan untuk pengujian dengan kernel RBF tidak jauh berbeda hanya saja masih di bawah dari kernel linear yaitu sebesar 71,89%.\n",
        "\n",
        "Dapat disimpulkan bahwa penggunaan kernel yang digunakan akan sangat berpengaruh terhadap hasil akurasi maka dari perlu untuk mengguji dengan berbagai kernel agar dapat mengetahui kernel terbaik."
      ],
      "metadata": {
        "id": "uzoVaMzN06rY"
      }
    }
  ]
}